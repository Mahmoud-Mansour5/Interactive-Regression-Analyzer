{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "id": "d61fd726",
      "cell_type": "markdown",
      "source": "\n# Custom Regression Web Application\n\nThis Jupyter Notebook demonstrates how a custom Linear Regression model (built from scratch using gradient descent) is integrated with a Flask-based web application for data upload, training, and prediction.\n\n---\n",
      "metadata": {}
    },
    {
      "id": "f4999606",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import os\nimport io\nimport uuid\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom flask import Flask, request, render_template, redirect, url_for, jsonify\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom werkzeug.utils import secure_filename\n\n# Use non-GUI backend for matplotlib\nimport matplotlib\nmatplotlib.use('Agg')\n",
      "outputs": []
    },
    {
      "id": "d1cbe5ff",
      "cell_type": "markdown",
      "source": "## Custom Linear Regression Class\nThis class implements a linear regression model using gradient descent.",
      "metadata": {}
    },
    {
      "id": "53c6ec60",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import numpy as np\n\nclass CustomLinearRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def fit(self, X, y):\n        \"\"\"\n        Train the linear regression model using gradient descent.\n        \n        Parameters:\n        X : numpy array of shape (n_samples, n_features)\n            Training data\n        y : numpy array of shape (n_samples,)\n            Target values\n        \"\"\"\n        # Convert inputs to numpy arrays and ensure float64 type\n        X = np.asarray(X, dtype=np.float64)\n        y = np.asarray(y, dtype=np.float64)\n        \n        # Initialize parameters\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features, dtype=np.float64)\n        self.bias = 0.0\n        \n        # Initialize best parameters (for early stopping)\n        best_weights = self.weights.copy()\n        best_bias = self.bias\n        best_cost = float('inf')\n        \n        # Gradient descent\n        for iteration in range(self.n_iterations):\n            try:\n                # Forward pass (make predictions)\n                y_predicted = self._predict(X)\n                \n                # Calculate gradients with numerical stability\n                error = y_predicted - y\n                dw = (1/n_samples) * np.dot(X.T, error)\n                db = (1/n_samples) * np.sum(error)\n                \n                # Clip gradients to prevent explosion\n                dw = np.clip(dw, -1e10, 1e10)\n                db = np.clip(db, -1e10, 1e10)\n                \n                # Update parameters\n                self.weights -= self.learning_rate * dw\n                self.bias -= self.learning_rate * db\n                \n                # Calculate cost\n                current_cost = self._compute_cost(y_predicted, y)\n                \n                # Store cost history\n                self.cost_history.append(current_cost)\n                \n                # Update best parameters if current cost is better\n                if current_cost < best_cost and not np.isnan(current_cost):\n                    best_cost = current_cost\n                    best_weights = self.weights.copy()\n                    best_bias = self.bias\n                \n                # Early stopping if cost is not improving\n                if len(self.cost_history) > 10:\n                    if np.mean(np.diff(self.cost_history[-10:])) > 0:\n                        break\n                \n            except (RuntimeWarning, RuntimeError) as e:\n                print(f\"Warning at iteration {iteration}: {str(e)}\")\n                break\n        \n        # Use best parameters found\n        self.weights = best_weights\n        self.bias = best_bias\n\n    def _predict(self, X):\n        \"\"\"\n        Make predictions using the current weights and bias.\n        \"\"\"\n        X = np.asarray(X, dtype=np.float64)\n        return np.dot(X, self.weights) + self.bias\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the linear regression model.\n        \n        Parameters:\n        X : numpy array of shape (n_samples, n_features)\n            Samples to predict\n        \n        Returns:\n        y_pred : numpy array of shape (n_samples,)\n            Predicted values\n        \"\"\"\n        try:\n            predictions = self._predict(X)\n            # Clip predictions to prevent extreme values\n            return np.clip(predictions, np.min(predictions[~np.isinf(predictions)]), \n                         np.max(predictions[~np.isinf(predictions)]))\n        except Exception as e:\n            print(f\"Error in predict: {str(e)}\")\n            return np.zeros(X.shape[0])\n\n    def _compute_cost(self, y_predicted, y):\n        \"\"\"\n        Compute the Mean Squared Error cost function with numerical stability.\n        \"\"\"\n        try:\n            n_samples = len(y)\n            errors = y_predicted - y\n            # Clip errors to prevent overflow\n            errors = np.clip(errors, -1e10, 1e10)\n            cost = (1/(2*n_samples)) * np.sum(errors ** 2)\n            return cost if not np.isnan(cost) else float('inf')\n        except Exception as e:\n            print(f\"Error in cost computation: {str(e)}\")\n            return float('inf')\n\n    def get_params(self):\n        \"\"\"\n        Get the model parameters.\n        \n        Returns:\n        dict : Dictionary containing the weights and bias\n        \"\"\"\n        return {\n            'weights': self.weights,\n            'bias': self.bias,\n            'cost_history': self.cost_history\n        }\n\n    def score(self, X, y):\n        \"\"\"\n        Calculate the R\u00b2 score (coefficient of determination).\n        \n        Parameters:\n        X : numpy array of shape (n_samples, n_features)\n            Test samples\n        y : numpy array of shape (n_samples,)\n            True values\n        \n        Returns:\n        score : float\n            R\u00b2 score\n        \"\"\"\n        try:\n            y_pred = self.predict(X)\n            ss_total = np.sum((y - np.mean(y)) ** 2)\n            ss_residual = np.sum((y - y_pred) ** 2)\n            r2 = 1 - (ss_residual / ss_total)\n            return r2 if not np.isnan(r2) else 0.0\n        except Exception as e:\n            print(f\"Error in score computation: {str(e)}\")\n            return 0.0 ",
      "outputs": []
    },
    {
      "id": "a9538336",
      "cell_type": "markdown",
      "source": "## Utility Functions\nThese functions handle plotting and data visualization.",
      "metadata": {}
    },
    {
      "id": "e3e24ad6",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def save_plot(fig):\n    filename = f\"{uuid.uuid4().hex}.png\"\n    path = os.path.join(IMAGE_FOLDER, filename)\n    fig.savefig(path, bbox_inches='tight')\n    plt.close(fig)\n    return f\"/static/images/{filename}\"\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef create_box_plot(X, y):\n    # Create box plots for first 4 features\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    axes = axes.ravel()\n    \n    for idx, feature in enumerate(X.columns[:4]):\n        axes[idx].boxplot(X[feature])\n        axes[idx].set_title(f'Box Plot of {feature}')\n        axes[idx].set_ylabel('Value')\n    \n    plt.tight_layout()\n    return fig\n\ndef create_scatter_matrix(X, y):\n    # Create scatter matrix for first 4 features\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    axes = axes.ravel()\n    \n    for idx, feature in enumerate(X.columns[:4]):\n        axes[idx].scatter(X[feature], y, alpha=0.6)\n        axes[idx].set_xlabel(feature)\n        axes[idx].set_ylabel('Target')\n        axes[idx].set_title(f'{feature} vs Target')\n    \n    plt.tight_layout()\n    return fig\n\ndef create_prediction_error_plot(y_test, predictions):\n    # Create prediction error plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    error = y_test - predictions\n    ax.scatter(predictions, error, alpha=0.6)\n    ax.axhline(y=0, color='r', linestyle='--')\n    ax.set_xlabel('Predicted Values')\n    ax.set_ylabel('Prediction Error')\n    ax.set_title('Prediction Error Plot')\n    return fig\n\ndef create_learning_curve_plot(X, y):\n    # Create simple learning curve by varying training set size\n    train_sizes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n    train_scores = []\n    test_scores = []\n    \n    for size in train_sizes:\n        try:\n            # Split the data\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-size)\n            \n            # Scale the features\n            X_mean = X_train.mean()\n            X_std = X_train.std().replace(0, 1)\n            X_train_scaled = (X_train - X_mean) / X_std\n            X_test_scaled = (X_test - X_mean) / X_std\n\n            # Convert to numpy arrays and ensure float64 type\n            X_train_scaled = X_train_scaled.values.astype('float64')\n            X_test_scaled = X_test_scaled.values.astype('float64')\n            y_train = y_train.values.astype('float64')\n            y_test = y_test.values.astype('float64')\n\n            # Train model\n            model = CustomLinearRegression(learning_rate=0.01, n_iterations=1000)\n            model.fit(X_train_scaled, y_train)\n            \n            # Make predictions\n            train_pred = model.predict(X_train_scaled)\n            test_pred = model.predict(X_test_scaled)\n            \n            # Calculate MSE, handling potential NaN values\n            train_mse = np.mean((y_train - train_pred) ** 2)\n            test_mse = np.mean((y_test - test_pred) ** 2)\n            \n            # Only append scores if they are valid numbers\n            if not (np.isnan(train_mse) or np.isnan(test_mse) or \n                   np.isinf(train_mse) or np.isinf(test_mse)):\n                train_scores.append(train_mse)\n                test_scores.append(test_mse)\n            \n        except Exception as e:\n            print(f\"Error at training size {size}: {str(e)}\")\n            continue\n    \n    # Create plot only if we have valid scores\n    if len(train_scores) > 0 and len(test_scores) > 0:\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.plot(train_sizes[:len(train_scores)], train_scores, 'o-', label='Training Error')\n        ax.plot(train_sizes[:len(test_scores)], test_scores, 'o-', label='Testing Error')\n        ax.set_xlabel('Training Set Size')\n        ax.set_ylabel('Mean Squared Error')\n        ax.set_title('Learning Curve')\n        ax.legend()\n        return fig\n    else:\n        # Create an empty plot with a message if no valid scores\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.text(0.5, 0.5, 'Could not generate learning curve due to numerical instability',\n                horizontalalignment='center', verticalalignment='center')\n        ax.set_xlabel('Training Set Size')\n        ax.set_ylabel('Mean Squared Error')\n        ax.set_title('Learning Curve')\n        return fig\n\ndef get_first_five_columns(X):\n    # Get the first 5 column names\n    return X.columns[:5].tolist()\n",
      "outputs": []
    },
    {
      "id": "cb9345fa",
      "cell_type": "markdown",
      "source": "## Data Preprocessing and Training\nThis function handles the CSV upload, preprocessing, training, and generating plots.",
      "metadata": {}
    },
    {
      "id": "0098d331",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def upload_file():\n    global current_model, feature_names, column_values, model_trained\n    \n    if 'file' not in request.files:\n        return render_template('upload.html', error=\"No file part\")\n    \n    file = request.files['file']\n    if file.filename == '':\n        return render_template('upload.html', error=\"No selected file\")\n    \n    if file and allowed_file(file.filename):\n        try:\n            filename = secure_filename(file.filename)\n            filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n            file.save(filepath)\n\n            # Load CSV\n            df = pd.read_csv(filepath)\n\n            # Capture original shape and info\n            original_shape = df.shape\n            buf_before = io.StringIO()\n            df.info(buf=buf_before)\n            original_info = buf_before.getvalue()\n\n            # Clean data\n            df.drop_duplicates(inplace=True)\n            df_cleaned = df.copy()\n\n            # Remove rows where target variable (MSRP) is NaN\n            df_cleaned = df_cleaned.dropna(subset=['MSRP'])\n\n            # Convert MSRP to numeric, removing any currency symbols or commas\n            df_cleaned['MSRP'] = pd.to_numeric(df_cleaned['MSRP'].astype(str).str.replace('[$,]', '', regex=True), errors='coerce')\n\n            # Separate features and target\n            X = df_cleaned.iloc[:, :-1]  # all columns except last\n            y = df_cleaned.iloc[:, -1]   # last column is target (MSRP)\n\n            # Get first 5 columns and their unique values\n            feature_names = get_first_five_columns(X)\n            column_values = {}\n            for col in X.columns:\n                # Get unique values\n                unique_values = X[col].unique()\n                # Sort numeric values numerically, strings alphabetically\n                if pd.api.types.is_numeric_dtype(X[col]):\n                    unique_values = sorted([x for x in unique_values if pd.notna(x)])\n                else:\n                    unique_values = sorted([x for x in unique_values if pd.notna(x)], key=str)\n                # Convert all values to strings for display\n                column_values[col] = [str(val) for val in unique_values]\n\n            # Store original feature names before encoding\n            original_features = X.columns.tolist()\n\n            # Identify numeric and categorical columns\n            numeric_features = []\n            categorical_features = []\n            \n            for col in X.columns:\n                # Try to convert to numeric\n                try:\n                    X[col] = pd.to_numeric(X[col], errors='raise')\n                    numeric_features.append(col)\n                except (ValueError, TypeError):\n                    categorical_features.append(col)\n\n            # Handle missing values in numeric columns\n            for col in numeric_features:\n                median_val = X[col].median()\n                X[col] = X[col].fillna(median_val)\n\n            # Handle missing values and encode categorical columns\n            for col in categorical_features:\n                # Fill missing values with mode\n                mode_val = X[col].mode()[0]\n                X[col] = X[col].fillna(mode_val)\n                # Convert to category type\n                X[col] = X[col].astype('category')\n\n            # One-hot encode categorical variables\n            X = pd.get_dummies(X, columns=categorical_features)\n\n            # Remove any remaining NaN values\n            X = X.fillna(0)\n\n            # Convert all columns to float64\n            X = X.astype('float64')\n\n            # Capture cleaned shape and info\n            cleaned_shape = X.shape\n            buf_after = io.StringIO()\n            X.info(buf=buf_after)\n            cleaned_info = buf_after.getvalue()\n\n            # Create data exploration plots\n            numeric_features_for_plots = X.select_dtypes(include=['float64', 'int64']).columns[:4]\n            box_plot_url = save_plot(create_box_plot(X[numeric_features_for_plots], y))\n            scatter_matrix_url = save_plot(create_scatter_matrix(X[numeric_features_for_plots], y))\n\n            # Train/test split and model\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n            \n            # Scale the features\n            X_mean = X_train.mean()\n            X_std = X_train.std().replace(0, 1)  # Replace zero std with 1 to avoid division by zero\n            X_train_scaled = (X_train - X_mean) / X_std\n            X_test_scaled = (X_test - X_mean) / X_std\n\n            # Convert to numpy arrays and ensure float64 type\n            X_train_scaled = X_train_scaled.values.astype('float64')\n            X_test_scaled = X_test_scaled.values.astype('float64')\n            y_train = y_train.values.astype('float64')\n            y_test = y_test.values.astype('float64')\n\n            # Train the model\n            model = CustomLinearRegression(learning_rate=0.01, n_iterations=1000)\n            model.fit(X_train_scaled, y_train)\n            \n            # Make predictions\n            predictions = model.predict(X_test_scaled)\n            mse = mean_squared_error(y_test, predictions)\n\n            # Store model and scaling parameters globally\n            current_model = {\n                'model': model,\n                'feature_names_in_': X.columns,\n                'X_mean': X_mean,\n                'X_std': X_std\n            }\n            model_trained = True\n\n            # Create model evaluation plots\n            error_plot_url = save_plot(create_prediction_error_plot(y_test, predictions))\n            learning_curve_url = save_plot(create_learning_curve_plot(X, y))\n\n            # Original plots\n            numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n            if len(numeric_cols) > 0:\n                first_feature = numeric_cols[0]\n                fig1, ax1 = plt.subplots()\n                ax1.scatter(X[first_feature], y, alpha=0.6)\n                ax1.set_title(f'{first_feature} vs Target (Before Training)')\n                ax1.set_xlabel(first_feature)\n                ax1.set_ylabel('Target')\n                plot1_url = save_plot(fig1)\n            else:\n                plot1_url = None\n\n            fig2, ax2 = plt.subplots()\n            ax2.scatter(y_test, predictions, alpha=0.6, color='green')\n            ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # ideal line\n            ax2.set_title('Actual vs Predicted (After Training)')\n            ax2.set_xlabel('Actual')\n            ax2.set_ylabel('Predicted')\n            plot2_url = save_plot(fig2)\n\n            return render_template('training_results.html',\n                                original_shape=original_shape,\n                                original_info=original_info,\n                                cleaned_shape=cleaned_shape,\n                                cleaned_info=cleaned_info,\n                                mse=mse,\n                                feature_names=feature_names,\n                                box_plot_url=box_plot_url,\n                                scatter_matrix_url=scatter_matrix_url,\n                                error_plot_url=error_plot_url,\n                                learning_curve_url=learning_curve_url,\n                                plot1_url=plot1_url,\n                                plot2_url=plot2_url)\n\n        except Exception as e:\n            import traceback\n            print(traceback.format_exc())  # Print the full error traceback\n            return render_template('upload.html', error=f\"Error processing file: {str(e)}\")\n    \n    return render_template('upload.html', error=\"Invalid file type. Please upload a CSV file.\")\n\nif __name__ == '__main__':\n    app.run(debug=True)",
      "outputs": []
    }
  ]
}